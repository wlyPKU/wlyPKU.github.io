<!doctype html>



  


<html class="theme-next pisces use-motion">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />












  
  
  <link href="/vendors/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/vendors/font-awesome/css/font-awesome.min.css?v=4.4.0" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.0.1" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="机器学习,优化算法," />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.0.1" />






<meta name="description" content="A brief introduction to stochastic gradient descent and its variants.">
<meta property="og:type" content="article">
<meta property="og:title" content="SGD 变种">
<meta property="og:url" content="http://wlyPKU.github.io/2016/10/06/SGD-variants/index.html">
<meta property="og:site_name" content="小小小宇">
<meta property="og:description" content="A brief introduction to stochastic gradient descent and its variants.">
<meta property="og:image" content="http://wlyPKU.github.io/2016/10/06/SGD-variants/without_momentum.jpg">
<meta property="og:image" content="http://wlyPKU.github.io/2016/10/06/SGD-variants/with_momentum.jpg">
<meta property="og:image" content="http://wlyPKU.github.io/2016/10/06/SGD-variants/nesterov.jpeg">
<meta property="og:image" content="http://wlyPKU.github.io/2016/10/06/SGD-variants/Nesterov_Momentum.jpg">
<meta property="og:image" content="http://wlyPKU.github.io/2016/10/06/SGD-variants/adadelta.jpg">
<meta property="og:image" content="http://wlyPKU.github.io/2016/10/06/SGD-variants/contours_evaluation_optimizers.gif">
<meta property="og:image" content="http://wlyPKU.github.io/2016/10/06/SGD-variants/saddle_point_evaluation_optimizers.gif">
<meta property="og:image" content="http://wlyPKU.github.io/2016/10/06/SGD-variants/1.png">
<meta property="og:image" content="http://wlyPKU.github.io/2016/10/06/SGD-variants/2.png">
<meta property="og:image" content="http://wlyPKU.github.io/2016/10/06/SGD-variants/3.png">
<meta property="og:updated_time" content="2016-10-08T08:12:13.925Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="SGD 变种">
<meta name="twitter:description" content="A brief introduction to stochastic gradient descent and its variants.">
<meta name="twitter:image" content="http://wlyPKU.github.io/2016/10/06/SGD-variants/without_momentum.jpg">



<script type="text/javascript" id="hexo.configuration">
  var NexT = window.NexT || {};
  var CONFIG = {
    scheme: 'Pisces',
    sidebar: {"position":"left","display":"post"},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: 0,
      author: '博主'
    }
  };
</script>




  <link rel="canonical" href="http://wlyPKU.github.io/2016/10/06/SGD-variants/"/>

  <title> SGD 变种 | 小小小宇 </title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  










  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container one-collumn sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-meta ">
  

  <div class="custom-logo-site-title">
    <a href="/"  class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <span class="site-title">小小小宇</span>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>
  <p class="site-subtitle"></p>
</div>

<div class="site-nav-toggle">
  <button>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
  </button>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup">
 <span class="search-icon fa fa-search"></span>
 <input type="text" id="local-search-input">
 <div id="local-search-result"></div>
 <span class="popup-btn-close">close</span>
</div>


    </div>
  
</nav>

 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                SGD 变种
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">发表于</span>
            <time itemprop="dateCreated" datetime="2016-10-06T19:41:27+08:00" content="2016-10-06">
              2016-10-06
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/机器学习/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
              <span class="post-comments-count">
                &nbsp; | &nbsp;
                <a href="/2016/10/06/SGD-variants/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2016/10/06/SGD-variants/" itemprop="commentsCount"></span>
                </a>
              </span>
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>A brief introduction to stochastic gradient descent and its variants.<br><a id="more"></a></p>
<h1 id="SGD模型"><a href="#SGD模型" class="headerlink" title="SGD模型"></a>SGD模型</h1><p>SGD指stochastic gradient descent, 即随机梯度下降,是梯度下降的batch版本.<br>对于训练数据集，我们首先将其分成n个batch，每个batch包含m个样本。我们每次更新都利用一个batch的数据，而非整个训练集。即：<br>\begin{equation}<br>\theta = \theta - \eta \cdot \nabla_\theta J( \theta; x^{(i)}; y^{(i)})<br>\end{equation}<br>其中, $\eta$ 为学习率，$g_t$为$x$在$t$时刻的梯度。<br>这么做的好处在于：</p>
<ol>
<li>当训练数据太多时，利用整个数据集更新往往时间上不显示。batch的方法可以减少机器的压力，并且可以更快地收敛。</li>
<li>当训练集有很多冗余时（类似的样本出现多次），batch方法收敛更快。以一个极端情况为例，若训练集前一半和后一半梯度相同。那么如果前一半作为一个batch，后一半作为另一个batch，那么在一次遍历训练集时，batch的方法向最优解前进两个step，而整体的方法只前进一个step。</li>
</ol>
<h1 id="非自适应算法"><a href="#非自适应算法" class="headerlink" title="非自适应算法"></a>非自适应算法</h1><h2 id="动量SGD"><a href="#动量SGD" class="headerlink" title="动量SGD"></a>动量SGD</h2><p>SGD方法的一个缺点是，其更新方向完全依赖于当前的batch，因而其更新十分不稳定。解决这一问题的一个简单的做法便是引入momentum。<br>momentum即动量，它模拟的是物体运动时的惯性，即更新的时候在一定程度上保留之前更新的方向，同时利用当前batch的梯度微调最终的更新方向。这样一来，可以在一定程度上增加稳定性，从而学习地更快，并且还有一定摆脱局部最优的能力：<br>\begin{equation}<br>v_t = \gamma v_{t-1} + \eta \nabla_\theta J( \theta)<br>\end{equation}<br>\begin{equation}<br>\theta = \theta - v_t<br>\end{equation}<br>其中，$\gamma$即momentum，表示要在多大程度上保留原来的更新方向，这个值在0-1之间，在训练开始时，由于梯度可能会很大，所以初始值一般选为0.5；当梯度不那么大时，改为0.9。$\eta$是学习率，即当前batch的梯度多大程度上影响最终更新方向，跟普通的SGD含义相同。$\gamma$ 与 $\eta$ 之和不一定为1。</p>
<div align="center"><br><img src="/2016/10/06/SGD-variants/without_momentum.jpg" alt="标准SGD" title="标准SGD"><br><img src="/2016/10/06/SGD-variants/with_momentum.jpg" alt="动量SGD" title="动量SGD"><br></div>

<p>SGD has trouble navigating ravines, i.e. areas where the surface curves much more steeply in one dimension than in another, which are common around local optima. In these scenarios, SGD oscillates across the slopes of the ravine while only making hesitant progress along the bottom towards the local optimum as in Image 1.<br>Momentum is a method that helps accelerate SGD in the relevant direction and dampens oscillations as can be seen in Image 2. It does this by adding a fraction γ of the update vector of the past time step to the current update vector:<br>\begin{equation}<br>v_t = \gamma v_{t-1} + \eta \nabla_\theta J( \theta)<br>\end{equation}<br>\begin{equation}<br>\theta = \theta - v_t<br>\end{equation}<br>Some implementations exchange the signs in the equations. The momentum term γ is usually set to 0.9 or a similar value.When cross-validated, this parameter is usually set to values such as [0.5, 0.9, 0.95, 0.99]. Similar to annealing schedules for learning rates (discussed later, below), optimization can sometimes benefit a little from momentum schedules, where the momentum is increased in later stages of learning. A typical setting is to start with momentum of about 0.5 and anneal it to 0.99 or so over multiple epochs.<br>Essentially, when using momentum, we push a ball down a hill. The ball accumulates momentum as it rolls downhill, becoming faster and faster on the way (until it reaches its terminal velocity if there is air resistance, i.e. γ&lt;1).<br>The same thing happens to our parameter updates: The momentum term increases for dimensions whose gradients point in the same directions and reduces updates for dimensions whose gradients change directions. As a result, we gain faster convergence and reduced oscillation.</p>
<h2 id="Nesterov-Momentum"><a href="#Nesterov-Momentum" class="headerlink" title="Nesterov Momentum"></a>Nesterov Momentum</h2><p>这是对传统momentum方法的一项改进，由Ilya Sutskever(2012 unpublished)在Nesterov工作的启发下提出的。<br>其基本思路如下图（转自Hinton的coursera公开课lecture 6a）:</p>
<div align="center"><br><img src="/2016/10/06/SGD-variants/nesterov.jpeg" alt="nesterov动量" title="nesterov动量"><br><img src="/2016/10/06/SGD-variants/Nesterov_Momentum.jpg" alt="nesterov动量" title="nesterov动量"><br></div>

<p>首先，按照原来的更新方向更新一步（棕色线），然后在该位置计算梯度值（红色线），然后用这个梯度值修正最终的更新方向（绿色线）。上图中描述了两步的更新示意图，其中蓝色线是标准momentum更新路径。</p>
<p>公式描述为：<br>\begin{equation}<br>v_t = \gamma v_{t-1} + \eta \nabla_\theta J( \theta - \gamma v_{t-1} )<br>\end{equation}<br>\begin{equation}<br>\theta = \theta - v_t<br>\end{equation}<br>However, a ball that rolls down a hill, blindly following the slope, is highly unsatisfactory. We’d like to have a smarter ball, a ball that has a notion of where it is going so that it knows to slow down before the hill slopes up again.\\<br>Nesterov accelerated gradient (NAG) is a way to give our momentum term this kind of prescience. We know that we will use our momentum term γvt−1 to move the parameters θ. Computing $\theta−\gamma v_t−1$ thus gives us an approximation of the next position of the parameters (the gradient is missing for the full update), a rough idea where our parameters are going to be. We can now effectively look ahead by calculating the gradient not w.r.t. to our current parameters θθ but w.r.t. the approximate future position of our parameters:<br>Like momentum,NAG is a first-order optimization method with better convergence rate guarantee than gradient descent in certain situations. In particular, for general smooth(non-strongly) convex functions and a deterministic gradient, NAG achieves a global convergence rate of O($1/T^2$) (versus the O(1/T) of gradient descent), with constant proportional to the Lipschitz coefficient of the derivative and the squared Euclidean distance to the solution. While NAG is not typically thought of as a type of momentum, it indeed turns out to be closely related to classical momentum, differing only in the precise update of the velocity vector v.</p>
<h2 id="learing-rate-选择"><a href="#learing-rate-选择" class="headerlink" title="learing rate 选择"></a>learing rate 选择</h2><h3 id="常数learning-rate"><a href="#常数learning-rate" class="headerlink" title="常数learning rate"></a>常数learning rate</h3><h3 id="折半下降"><a href="#折半下降" class="headerlink" title="折半下降"></a>折半下降</h3><p>每若干轮降低下learning rate,如每5轮降低一半,或者每20轮降低0.1<br>Reduce the learning rate by some factor every few epochs. Typical values might be reducing the learning rate by a half every 5 epochs, or by 0.1 every 20 epochs. These numbers depend heavily on the type of problem and the model. One heuristic you may see in practice is to watch the validation error while training with a fixed learning rate, and reduce the learning rate by a constant (e.g. 0.5) whenever the validation error stops improving. \par</p>
<h3 id="指数下降"><a href="#指数下降" class="headerlink" title="指数下降"></a>指数下降</h3><p>\begin{equation}<br>\alpha = \frac{\alpha_0}{e^{kt}}<br>\end{equation}<br>其中是$k$超参数,$t$是迭代轮数</p>
<h3 id="frac-1-t-下降"><a href="#frac-1-t-下降" class="headerlink" title="$\frac{1}{t}$下降"></a>$\frac{1}{t}$下降</h3><p>\begin{equation}<br>\alpha = \frac{\alpha_0}{1+kt}<br>\end{equation}<br>数学形式,$k$是超参数,$t$是迭代轮数。</p>
<h1 id="自适应算法"><a href="#自适应算法" class="headerlink" title="自适应算法"></a>自适应算法</h1><h2 id="Adagrad"><a href="#Adagrad" class="headerlink" title="Adagrad"></a>Adagrad</h2><p>上面提到的方法对于所有参数都使用了同一个更新速率。但是同一个更新速率不一定适合所有参数。比如有的参数可能已经到了仅需要微调的阶段，但又有些参数由于对应样本少等原因，还需要较大幅度的调动。<br>Adagrad就是针对这一问题提出的，自适应地为各个参数分配不同学习率的算法。其公式如下：<br>\begin{equation}<br>\theta_{t+1} = \theta_{t} - \dfrac{\eta}{\sqrt{G_{t} + \epsilon}} \odot g_{t}<br>\end{equation}<br>其中$G^{t}\in R^{d\times d}$是一个对角矩阵，其中<br>\begin{equation}<br>G_{i, i} = \sqrt{\sum_{T=1}^tg_{T,i}^2}<br>\end{equation}<br>而$\epsilon$是一个平滑系数，使得不会出现除零的现象，通常在$10^{-8}$量级。Adagrad的一个主要优势是考虑了不同参数可能处在更新的不同阶段，同时避免了调参的问题，通常可以取 $\eta$为0.01.<br>Adagrad的一个主要缺点在于其参数的平方和累加：因为随着训练轮数的增加，每一个累加项都是正数，这使得整体的学习速率最终会变得非常的小,最终模型会静止不动(可能尚未到达收敛点).</p>
<h2 id="Adadelta"><a href="#Adadelta" class="headerlink" title="Adadelta"></a>Adadelta</h2><p>Adagrad算法存在三个问题<br>1.其学习率是单调递减的，训练后期学习率非常小<br>2.其需要手工设置一个全局的初始学习率<br>3.更新时，左右两边的单位不同一<br>Adadelta针对上述三个问题提出了比较漂亮的解决方案。其公式如下：<br>\begin{equation}<br>E[g^2]_t = \gamma E[g^2]_{t-1} + (1 - \gamma) g^2_t<br>\end{equation}<br>\begin{equation}<br>E[\Delta \theta^2]_t = \gamma E[\Delta \theta^2]_{t-1} + (1 - \gamma) \Delta \theta^2_t<br>\end{equation}<br>\begin{equation}<br>RMS[\Delta \theta]_{t} = \sqrt{E[\Delta \theta^2]_t + \epsilon}\end{equation}<br>\begin{equation}<br>\Delta \theta_t = - \dfrac{RMS[\Delta \theta]_{t-1}}{RMS[g]_{t}} g_{t}<br>\end{equation}<br>\begin{equation}<br>\theta_{t+1} = \theta_t + \Delta \theta_t<br>\end{equation}<br>$\gamma$的值近似在0.9左右，同时$\epsilon$是一个平滑系数，使得不会出现除零的现象，通常在$10^{-8}$量级.<br>可以看到，如此一来adagrad中分子部分需要人工设置的初始学习率也消失了。</p>
<p>Experiment: neural network MINIST$ ADADELTA &gt; ADAGRAD &gt; MOMENTUM &gt; SGD$</p>
<div align="center"><br><img src="/2016/10/06/SGD-variants/adadelta.jpg" alt="实验比较" title="实验比较"><br></div>

<h2 id="RMSprop"><a href="#RMSprop" class="headerlink" title="RMSprop"></a>RMSprop</h2><p>RMSprop是一个未出版的，自适应学习速率方法，由Geoff Hinton提出。RMSprop和Adadelta的提出都源自于Adagrad，目的是为了解决Adagrad学习速率在迭代若干次后彻底消失的问题。本质上而言，其与Adadelta相同。<br>\begin{equation}<br>E[g^2]_t = 0.9 E[g^2]_{t-1} + 0.1 g^2_t<br>\end{equation}<br>\begin{equation}<br>\theta_{t+1} = \theta_{t} - \dfrac{\eta}{\sqrt{E[g^2]_t + \epsilon}} g_{t}<br>\end{equation}<br>Hinton建议设置$\epsilon$为0.9，而$\eta$的学习速率为0.001.</p>
<h2 id="adam"><a href="#adam" class="headerlink" title="adam"></a>adam</h2><p>\begin{equation}<br>m_t = \beta_1 m_{t-1} + (1 - \beta_1) g_t<br>\end{equation}<br>\begin{equation}<br>v_t = \beta_2 v_{t-1} + (1 - \beta_2) g_t^2<br>\end{equation}<br>\begin{equation}<br>\hat{m}_t = \dfrac{m_t}{1 - \beta^t_1}<br>\end{equation}<br>\begin{equation}<br>\hat{v}_t = \dfrac{v_t}{1 - \beta^t_2}<br>\end{equation}<br>\begin{equation}<br>\theta_{t+1} = \theta_{t} - \dfrac{\eta}{\sqrt{\hat{v}_t} + \epsilon} \hat{m}_t<br>\end{equation}<br>该方法和RMSProp唯一的区别是smooth过程,这里使用的是m来做smooth操作,而不是使用原始的gradient vector. 论文中推荐的超参数为eps=1e-6, bata1=0.9, beta2=0.999, 在实践中, 如果没有其他的特殊理由,一般推荐使用Adam方法, 并且, Adam算法通常会比RMSProp算法效果好. 另外,也可以尝试SGD+Nesterov Momentum. 完整的Adam算法中还包括bias的纠正机制, 这事因为,在刚开始的几个steps中,m和v都要初始化, 并且在warm up 之前他们都biased at zero.<br>更多的细节可以参考论文原文.<br>论文:The method computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients; the name Adam is derived from adaptive moment estimation. Our method is designed to combine the advantages of two recently popular methods: AdaGrad (Duchi et al., 2011), which works well with sparse gradients, and RMSProp (Tieleman &amp; Hinton, 2012), which works well in on-line and non-stationary settings; important connections to these and other stochastic optimization methods are clarified in section 5.Some of Adam’s advantages are that the magnitudes of parameter updates are invariant to rescaling of the gradient, its stepsizes are approximately bounded by the stepsize hyperparameter, it does not require a stationary objective, it works with sparse gradients, and it naturally performs a form of step size annealing.</p>
<h2 id="Adamax"><a href="#Adamax" class="headerlink" title="Adamax"></a>Adamax</h2><ul>
<li><a href="https://arxiv.org/pdf/1412.6980v8.pdf" target="_blank" rel="external">ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION</a><br>Adamax是Adam的一种变体，此方法对学习率的上限提供了一个更简单的范围。公式上的变化如下：<br>\begin{equation}<br>m_t = \beta_1 m_{t-1} + (1 - \beta_1) g_t<br>\end{equation}<br>\begin{equation}<br>v_t = max(\beta_2 \times v_{t-1}, \left| g_t\right|)<br>\end{equation}<br>\begin{equation}<br>\hat{m}_t = \dfrac{m_t}{1 - \beta^t_1}<br>\end{equation}<br>\begin{equation}<br>\theta_{t+1} = \theta_{t} - \dfrac{\eta}{v_t + \epsilon} \hat{m}_t<br>\end{equation}<br>可以看出，Adamax学习率的边界范围更简单.<br>parameter: Keras recommend:<br>Good default settings for the tested machine learning problems are $\eta= 0.002$, $\beta_1 = 0.9$ and $\beta_2= 0.999$. With $\beta_1^t$ we denote $\beta_1$ to the power $t$. Here, $\frac{\eta}{1-\beta_1^t}$is the learning rate with the bias-correction term for the first moment. <h2 id="Nadam"><a href="#Nadam" class="headerlink" title="Nadam"></a>Nadam</h2></li>
<li><a href="http://cs229.stanford.edu/proj2015/054_report.pdf" target="_blank" rel="external">Incorporating Nesterov Momentum into Adam</a></li>
</ul>
<p>Nadam类似于带有Nesterov动量项的Adam。公式如下：<br>\begin{equation}<br>\hat{g_t}=\frac{g_t}{1-\Pi_{i=1}^t\beta_{1i}}<br>\end{equation}<br>\begin{equation}<br>m_t=\beta_{1t} \times m_{t-1}+(1-\beta_{1t})\times g_t<br>\end{equation}<br>\begin{equation}<br>\hat{m_t}=\frac{m_t}{1-\Pi_{i=1}^{t+1}\beta_{1i}}<br>\end{equation}<br>\begin{equation}<br>n_t=\beta_2\times n_{t-1}+(1-\beta_2)\times g_t^2<br>\end{equation}<br>\begin{equation}<br>\bar{m_t}=(1-\beta_{1t})\times \hat{g_t}+\beta_{1(t+1)}\times \hat{m_t}<br>\end{equation}<br>\begin{equation}<br>\hat{n_t}=\frac{n_t}{1-\beta_{2}^t}<br>\end{equation}<br>\begin{equation}<br>\Delta{\theta_t}=-\eta*\frac{\bar{m_t}}{\sqrt{\hat{n_t}}+\epsilon}<br>\end{equation}<br>可以看出，Nadam对学习率有了更强的约束，同时对梯度的更新也有更直接的影响。一般而言，在想使用带动量的RMSprop，或者Adam的地方，大多可以使用Nadam取得更好的效果。</p>
<p>parameter: Keras recommend: lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=1e-08, schedule_decay=0.004<br><strong>All algorithms used $\beta_2$ = .999and $\epsilon$ = 1e−8 as suggested, with a momentum schedule given by $\beta_{1t} = \beta_1\times (1−0.5\times 0.96^{\frac{t}{250}})$ with $\beta_1$ = .99</strong></p>
<h1 id="对比"><a href="#对比" class="headerlink" title="对比"></a>对比</h1><p>经验之谈</p>
<ul>
<li>对于稀疏数据，尽量使用学习率可自适应的优化方法，不用手动调节，而且最好采用默认值</li>
<li>SGD通常训练时间更长，但是在好的初始化和学习率调度方案的情况下，结果更可靠</li>
<li>如果在意更快的收敛，并且需要训练较深较复杂的网络时，推荐使用学习率自适应的优化方法。</li>
<li>Adadelta，RMSprop，Adam是比较相近的算法，在相似的情况下表现差不多。</li>
<li>在想使用带动量的RMSprop，或者Adam的地方，大多可以使用Nadam取得更好的效果</li>
</ul>
<img src="/2016/10/06/SGD-variants/contours_evaluation_optimizers.gif" alt="SGD optimization on loss surface contours" title="SGD optimization on loss surface contours">
<img src="/2016/10/06/SGD-variants/saddle_point_evaluation_optimizers.gif" alt="SGD optimization on saddle point" title="SGD optimization on saddle point">
<p>Experiment: </p>
<ul>
<li>MNIST Logistic regression<br>Adam&gt; Nesterov &gt; AdaGrad ( converage cost)</li>
<li>IMDB BoW feature logsitic regression<br> $ Adam \approx Adagrad \approx RMSProp \approx Nesterov$</li>
<li>deepLearning </li>
</ul>
<div align="center"><br><img src="/2016/10/06/SGD-variants/1.png" alt="实验结果1" title="实验结果1"><br><img src="/2016/10/06/SGD-variants/2.png" alt="实验结果2" title="实验结果2"><br><img src="/2016/10/06/SGD-variants/3.png" alt="实验结果3" title="实验结果3"><br></div>

<p>MINST: Adagrad/Adadelta并不需要设置学习速率，相对安全,但调好参数的SGD+动量表现也非常出色。<br>原文:This demo lets you evaluate multiple trainers against each other on MNIST. By default I’ve set up a little benchmark that puts SGD/SGD with momentum/Adagrad/Adadelta/Nesterov against each other. For reference math and explanations on these refer to Matthew Zeiler’s Adadelta paper (Windowgrad is Idea #1 in the paper). In my own experience, Adagrad/Adadelta are “safer” because they don’t depend so strongly on setting of learning rates (with Adadelta being slightly better), but well-tuned SGD+Momentum almost always converges faster and at better final values.</p>
<p><strong>参考</strong><br><a href="http://sebastianruder.com/optimizing-gradient-descent/" target="_blank" rel="external">http://sebastianruder.com/optimizing-gradient-descent/</a><br><a href="https://keras.io/optimizers/" target="_blank" rel="external">https://keras.io/optimizers/</a><br><a href="http://cs231n.github.io/neural-networks-3/#sgd" target="_blank" rel="external">http://cs231n.github.io/neural-networks-3/#sgd</a><br><a href="https://en.wikipedia.org/wiki/Stochastic_gradient_descent" target="_blank" rel="external">https://en.wikipedia.org/wiki/Stochastic_gradient_descent</a><br><a href="http://blog.csdn.net/luo123n/article/details/48239963" target="_blank" rel="external">http://blog.csdn.net/luo123n/article/details/48239963</a><br><a href="http://blog.csdn.net/majordong100/article/details/51428642" target="_blank" rel="external">http://blog.csdn.net/majordong100/article/details/51428642</a><br><a href="http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf" target="_blank" rel="external">http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf</a><br><a href="http://cs.stanford.edu/people/karpathy/convnetjs/demo/trainers.html" target="_blank" rel="external">http://cs.stanford.edu/people/karpathy/convnetjs/demo/trainers.html</a><br><strong>Paper</strong><br>Unit Tests for Stochastic Optimization<br>ADADELTA: AN ADAPTIVE LEARNING RATE METHOD<br>ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION<br>Incorporating Nesterov Momentum into Adam</p>

      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        

      
    </div>

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/机器学习/" rel="tag">#机器学习</a>
          
            <a href="/tags/优化算法/" rel="tag">#优化算法</a>
          
        </div>
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2016/10/06/Downpour-SGD/" rel="next" title="Downpour SGD(大规模分布式深度网络)">
                <i class="fa fa-chevron-left"></i> Downpour SGD(大规模分布式深度网络)
              </a>
            
          </div>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2016/10/07/Parallelized-SGD/" rel="prev" title="Parallel SGD">
                Parallel SGD <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </article>



    <div class="post-spread">
      
        <div class="ds-share flat" data-thread-key="2016/10/06/SGD-variants/"
     data-title="SGD 变种"
     data-content=""
     data-url="http://wlyPKU.github.io/2016/10/06/SGD-variants/">
  <div class="ds-share-inline">
    <ul  class="ds-share-icons-16">

      <li data-toggle="ds-share-icons-more"><a class="ds-more" href="javascript:void(0);">分享到：</a></li>
      <li><a class="ds-weibo" href="javascript:void(0);" data-service="weibo">微博</a></li>
      <li><a class="ds-qzone" href="javascript:void(0);" data-service="qzone">QQ空间</a></li>
      <li><a class="ds-qqt" href="javascript:void(0);" data-service="qqt">腾讯微博</a></li>
      <li><a class="ds-wechat" href="javascript:void(0);" data-service="wechat">微信</a></li>

    </ul>
    <div class="ds-share-icons-more">
    </div>
  </div>
</div>
      
    </div>
  </div>


          </div>
          


          
  <div class="comments" id="comments">
    
      <div class="ds-thread" data-thread-key="2016/10/06/SGD-variants/"
           data-title="SGD 变种" data-url="http://wlyPKU.github.io/2016/10/06/SGD-variants/">
      </div>
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel ">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="http://ww3.sinaimg.cn/mw690/770b0925jw8f39az302pwj20w00w0dnd.jpg"
               alt="wlyPKU" />
          <p class="site-author-name" itemprop="name">wlyPKU</p>
          <p class="site-description motion-element" itemprop="description"></p>
        </div>
        <nav class="site-state motion-element">
          <div class="site-state-item site-state-posts">
            <a href="/archives">
              <span class="site-state-item-count">14</span>
              <span class="site-state-item-name">日志</span>
            </a>
          </div>

          
            <div class="site-state-item site-state-categories">
              <a href="/categories">
                <span class="site-state-item-count">6</span>
                <span class="site-state-item-name">分类</span>
              </a>
            </div>
          

          
            <div class="site-state-item site-state-tags">
              <a href="/tags">
                <span class="site-state-item-count">8</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="https://github.com/wlyPKU" target="_blank" title="GitHub">
                  
                    <i class="fa fa-fw fa-github"></i>
                  
                  GitHub
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="http://weibo.com/wangly2011/profile" target="_blank" title="Weibo">
                  
                    <i class="fa fa-fw fa-weibo"></i>
                  
                  Weibo
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="https://www.facebook.com/wangly.cs" target="_blank" title="Facebook">
                  
                    <i class="fa fa-fw fa-globe"></i>
                  
                  Facebook
                </a>
              </span>
            
          
        </div>

        
        

        
        

      </section>

      
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">
            
              
            
            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#SGD模型"><span class="nav-number">1.</span> <span class="nav-text">SGD模型</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#非自适应算法"><span class="nav-number">2.</span> <span class="nav-text">非自适应算法</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#动量SGD"><span class="nav-number">2.1.</span> <span class="nav-text">动量SGD</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Nesterov-Momentum"><span class="nav-number">2.2.</span> <span class="nav-text">Nesterov Momentum</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#learing-rate-选择"><span class="nav-number">2.3.</span> <span class="nav-text">learing rate 选择</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#常数learning-rate"><span class="nav-number">2.3.1.</span> <span class="nav-text">常数learning rate</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#折半下降"><span class="nav-number">2.3.2.</span> <span class="nav-text">折半下降</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#指数下降"><span class="nav-number">2.3.3.</span> <span class="nav-text">指数下降</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#frac-1-t-下降"><span class="nav-number">2.3.4.</span> <span class="nav-text">$\frac{1}{t}$下降</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#自适应算法"><span class="nav-number">3.</span> <span class="nav-text">自适应算法</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Adagrad"><span class="nav-number">3.1.</span> <span class="nav-text">Adagrad</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Adadelta"><span class="nav-number">3.2.</span> <span class="nav-text">Adadelta</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#RMSprop"><span class="nav-number">3.3.</span> <span class="nav-text">RMSprop</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#adam"><span class="nav-number">3.4.</span> <span class="nav-text">adam</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Adamax"><span class="nav-number">3.5.</span> <span class="nav-text">Adamax</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Nadam"><span class="nav-number">3.6.</span> <span class="nav-text">Nadam</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#对比"><span class="nav-number">4.</span> <span class="nav-text">对比</span></a></li></ol></div>
            
          </div>
        </section>
      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy;  2016 - 
  <span itemprop="copyrightYear">2017</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">wlyPKU</span>
</div>


        

        
      </div>
    </footer>

    <div class="back-to-top">
      <i class="fa fa-arrow-up"></i>
    </div>
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  



  
  <script type="text/javascript" src="/vendors/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/vendors/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/vendors/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/vendors/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/vendors/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/vendors/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.0.1"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.0.1"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.0.1"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.0.1"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.0.1"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.0.1"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.0.1"></script>



  

  
    
  

  <script type="text/javascript">
    var duoshuoQuery = {short_name:"wlypku"};
    (function() {
      var ds = document.createElement('script');
      ds.type = 'text/javascript';ds.async = true;
      ds.id = 'duoshuo-script';
      ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
      ds.charset = 'UTF-8';
      (document.getElementsByTagName('head')[0]
      || document.getElementsByTagName('body')[0]).appendChild(ds);
    })();
  </script>

  
    
    <script src="/vendors/ua-parser-js/dist/ua-parser.min.js?v=0.7.9"></script>
    <script src="/js/src/hook-duoshuo.js"></script>
  






  
  
  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length == 0) {
       search_path = "search.xml";
    }
    var path = "/" + search_path;
    // monitor main search box;

    function proceedsearch() {
      $("body").append('<div class="popoverlay">').css('overflow', 'hidden');
      $('.popup').toggle();

    }
    // search function;
    var searchFunc = function(path, search_id, content_id) {
    'use strict';
    $.ajax({
        url: path,
        dataType: "xml",
        async: true,
        success: function( xmlResponse ) {
            // get the contents from search data
            isfetched = true;
            $('.popup').detach().appendTo('.header-inner');
            var datas = $( "entry", xmlResponse ).map(function() {
                return {
                    title: $( "title", this ).text(),
                    content: $("content",this).text(),
                    url: $( "url" , this).text()
                };
            }).get();
            var $input = document.getElementById(search_id);
            var $resultContent = document.getElementById(content_id);
            $input.addEventListener('input', function(){
                var matchcounts = 0;
                var str='<ul class=\"search-result-list\">';
                var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                $resultContent.innerHTML = "";
                if (this.value.trim().length > 1) {
                // perform local searching
                datas.forEach(function(data) {
                    var isMatch = true;
                    var content_index = [];
                    var data_title = data.title.trim().toLowerCase();
                    var data_content = data.content.trim().replace(/<[^>]+>/g,"").toLowerCase();
                    var data_url = data.url;
                    var index_title = -1;
                    var index_content = -1;
                    var first_occur = -1;
                    // only match artiles with not empty titles and contents
                    if(data_title != '' && data_content != '') {
                        keywords.forEach(function(keyword, i) {
                            index_title = data_title.indexOf(keyword);
                            index_content = data_content.indexOf(keyword);
                            if( index_title < 0 && index_content < 0 ){
                                isMatch = false;
                            } else {
                                if (index_content < 0) {
                                    index_content = 0;
                                }
                                if (i == 0) {
                                    first_occur = index_content;
                                }
                            }
                        });
                    }
                    // show search results
                    if (isMatch) {
                        matchcounts += 1;
                        str += "<li><a href='"+ data_url +"' class='search-result-title'>"+ data_title +"</a>";
                        var content = data.content.trim().replace(/<[^>]+>/g,"");
                        if (first_occur >= 0) {
                            // cut out 100 characters
                            var start = first_occur - 20;
                            var end = first_occur + 80;
                            if(start < 0){
                                start = 0;
                            }
                            if(start == 0){
                                end = 50;
                            }
                            if(end > content.length){
                                end = content.length;
                            }
                            var match_content = content.substring(start, end);
                            // highlight all keywords
                            keywords.forEach(function(keyword){
                                var regS = new RegExp(keyword, "gi");
                                match_content = match_content.replace(regS, "<b class=\"search-keyword\">"+keyword+"</b>");
                            });

                            str += "<p class=\"search-result\">" + match_content +"...</p>"
                        }
                        str += "</li>";
                    }
                })};
                str += "</ul>";
                if (matchcounts == 0) { str = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>' }
                if (keywords == "") { str = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>' }
                $resultContent.innerHTML = str;
            });
            proceedsearch();
        }
    });}

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched == false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };

    });

    $('.popup-btn-close').click(function(e){
      $('.popup').hide();
      $(".popoverlay").remove();
      $('body').css('overflow', '');
    });
    $('.popup').click(function(e){
      e.stopPropagation();
    });
  </script>


  
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
  </script>

  <script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
      for (i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
      }
    });
  </script>
  <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->


  

  

</body>
</html>
